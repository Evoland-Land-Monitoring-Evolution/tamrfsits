#+TITLE: Temporal Attention, Multi-Resolution Fusion of Satellite Image Time-Series

This repository holds the model and training scripts related to Temporal Attention, Multi-Resolution Fusion (TAMRF) of Satellite Image Time-Series .

[[file:artwork/tamrfsits_output_examples.png]]

Illustration of the TAMRF model capabilities over Area Of Interest 31TFJ for year 2022. The model receives 25 Landsat observations and 40 Sentinel-2 observations as inputs (highlighted in *green*), and is asked to predict Landsat and Sentinel-2 at different query dates (highlighted in *red*). For some query dates, additional Sentinel-2 or Landsat images (e.g. not part of the model inputs) serve as reference data to compare the prediction with (highlighted in *blue*). Depending on the availability of clear or cloudy Sentinel2 or Landsat image in the input, several cases are presented. From top to bottom: row 1 shows Landsat prediction for a clear Landsat input date, row 2 shows Sentinel-2 predictions for a clear Sentinel-2 input date, row 3 shows Sentinel-2 prediction compared to true Sentinel-2 image for a clear Landsat input date, row 4 shows Sentinel-2 and Landsat prediction for a cloudy Landsat input date, and row 5 shows Sentinel-2 prediction compared to true Sentinel-2 image for a date when neither Landsat nor Sentinel-2 are seen by the model. Note that the model does not use cloud masks.

* In a nutshell

[[file:artwork/tamrfsits.png]]

*TAMRF* addresses the fusion of Satellite Image Time Series (SITS) of different spatial resolution and acquisition times. In contrast with methods from SoTA, *TAMRF* predicts all bands from both sensors, at all dates and at best spatial resolution. Given two Satellite Image Time Series over the same area with different spatial resolution and temporal sampling, *TAMRF* produces a latent reprensation, from which *TAMRF* can reconstruct arbitrary query dates, at the maximum spatial resolution, and free of any cloud! *TAMRF* follows an encoder - decoder scheme. The encoder comprises a datewise spatial encoder followed by a pixelwise temporal transformer encoder. The series of token from both sensors are informed with temporal positional encoding and learnable sensor tokens. The decoder receives the latent series of token and performs temporal cross-attention by means of a temporal transformer decoder and a series of temporal queries. A final datewise spatial decoder predicts all bands from both sensors.

*TAMRF* relies on several important innovations:
- A complete and generic formulation of the SITS fusion problem,
- A training scheme based on an advanced Masked Auto-Encoder strategy and new loss terms to favor correct temporal and spatial reconstruction,
- A DL architecture using standard blocks from the litterature (RRDBNet, Transformer, Temporal Positional Encoding) allowing to implement the proposed general formulation.


Find out more by reading the paper:
#+BEGIN_QUOTE
Julien Michel, Jordi Inglada. Temporal Attention Multi-Resolution Fusion of Satellite Image Time-Series, applied to Landsat-8/9 and Sentinel-2: all bands, any time, at best spatial resolution. 2025. ⟨[[https://hal.science/hal-05101526v2][[hal-05101526v2]]⟩
#+END_QUOTE


* Installation

This project uses [[https://pixi.sh][pixi]] as package manager and project configuration tool. Install =pixi= like this:

#+begin_src shell
curl -fsSL https://pixi.sh/install.sh | bash
#+end_src

Clone the =tamrfsits= sources like this:
#+begin_src shell
git clone https://github.com/Evoland-Land-Monitoring-Evolution/tamrfsits
#+end_src

And use =pixi= to install the project and its dependencies:

#+begin_src shell
cd tamrfsits
pixi install
#+end_src

Environment can be activated by using:

#+begin_src shell
pixi shell
#+end_src

* Code organization

** Project layout

The project is organised into the following sub-folders:

- ~src~ : python modules used in the project,
- ~hydra~ : hydra configuration to run the experiments,
- ~bin~ : main training, testing and model exporting scripts,
- ~slurm~ : sample slurm job files to run the training on a slurm-based infrastructure,
- ~tests~ :  ~pytest~ tests

** Code Quality

A ~Makefile~ is provided to ease the use of code quality control tools. Among the interesting targets are:

- ~make pylint-core~ : ~pylint~ checks  ~src~ folder (graded 9.96/10)
- ~make pylint~ : ~pylint~ checks whole project (graded 9.03/10)
- ~make mypy~ : ~mypy~ checks whole project (0 issue reported)
- ~make test-smoke~ : ~pytest~ runs all tests that do not require data or gpu
- ~make test-hydra-smoke~ : ~pytest~ runs ~hydra~ configuration tests that do not require the dataset
- ~make test-hydra-all~ : ~pytest~ runs all ~hydra~ configuration tests
- ~make test-all~ : ~pytest~ runs all tests

A few environment variables are required to run some of the tests:

- ~LS2S2_DATASET_PATH~ : Path to the LS2S2 dataset
- ~TESTS_OUPTUT_PATH~ : A temporary folder where tests can spill data


* Usage
** Download the LS2S2 dataset

The Landsat to Sentinel2 dataset (LS2S2) is available [[https://doi.org/10.5281/zenodo.15471890][on *Zenodo*]]:

#+begin_quote
MICHEL, J. (2025). Landsat to Sentinel-2 (LS2S2), a dataset for the fusion of joint Landsat and Sentinel-2 Satellite Image Time Series (1.0.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.15471890
#+end_quote

The Landsat To Sentinel2 dataset, multi-year, world-wide (LS2S2MYWW) is available at *Service de Données de l'OMP (SEDOO)* here https://doi.org/10.6096/1029.

Note that a ~Dataset~ class for LS2S2 following [[https://pytorch.org/][pyTorch]] standard, as well as a ~DataModule~ following [[https://lightning.ai/pytorch-lightning][pyTorch-Lightning]] conventions are available in [[file:src/tamrfsits/data/]].

** Download additional data (optional)

Additional data can be downloaded using the same script that has been used to download the LS2S2 dataset. Note that this requires an OpenEO account on Terrascope intance ([[https://openeo.vito.be/openeo/1.2]]).

Activate the download environment:

#+begin_src shell
pixi shell
#+end_src

Prepare a json file describing your AOI, by adapting the following example:

#+begin_src json
{
    "name": "34TFS_24",
    "aoi": [
        610410.0,
        5148960.0,
        620310.0,
        5158860.0
    ],
    "crs": "EPSG:32634",
    "start_date": "2022-01-01",
    "end_date": "2023-01-01"
}
#+end_src

Json files for the LS2S2 dataset can be found in [[file:data/aois/]].

In order to download the data, the first step is to submit the jobs to OpenEO:

#+begin_src shell
python bin/sample_ls2s2_fusion.py --json myaoi.json --output path_to_output_folder --mode submit
#+end_src

Once jobs are finished, we can run the download step which retrieves all results as NetCDF files:

#+begin_src shell
python bin/sample_ls2s2_fusion.py --json myaoi.json --output path_to_output_folder --mode download
#+end_src

Final step is to extract the NetCDF content to separate GeoTIFF:

#+begin_src shell
python bin/sample_ls2s2_fusion.py --json myaoi.json --output path_to_output_folder --mode extract
#+end_src

** Train the model (optional)

Training can be achieved by means of the training script, once path to dataset is set in hydra configuration : [[hydra/locatation]].

First, activate the default environment:

#+begin_src shell
pixi shell
#+end_src

Then run the training script using the default hydra configuration:

#+begin_src shell
PYTHONOPTIMIZE=TRUE PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python bin/train.py
#+end_src


Training is time consuming, and is best performed on a High Performance Computing Infrastructure. Here is an example of slurm job used to train TAMRF on the CNES infrastructure:

#+begin_src shell
#!/bin/bash
#SBATCH --output=slurm-logs/%j.out
#SBATCH --error=slurm-logs/%j.err
#SBATCH --partition=gpu_h100
#SBATCH --signal=SIGUSR1@90
#SBATCH --qos=gpu_h100
#SBATCH -N 1                        # number of nodes ( or --nodes=1)
#SBATCH --gres=gpu:1                # number of gpus
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=16
#SBATCH --time=24:00:00             # Walltime 24h
#SBATCH --mem-per-cpu=11G           # memory per cpu
#SBATCH --export=none              #  to start the job with a clean environnement and source of ~/.bashrc
#SBATCH --open-mode=append
cd src/tamrfsits/
conda activate .pixi/envs/default/
cd bin
export PYTHONOPTIMIZE=TRUE
export TORCHINDUCTOR_CACHE_DIR=$TMPDIR/$SLURM_JOBID/
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True srun --export=ALL python ./train.py "$@" location=trex datamodule.config.num_workers=16 datamodule.config.cache_dir=$TMPDIR/$SLURM_JOBID/
#+end_src

*Note:*
- Training is intensive and requires at least a single NVIDIA A100 GPU with 80 Go of memory.
- The ~PYTHONOPTIMIZE=TRUE~ environment variable disables the many assert in the code, enabling faster training,
- The ~PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True~ environment variable allows to squeeze more data in the GPU memory.
- The ~--signal=SIGUSR1@90~ option enables auto-requeue, which allows to continue training past the lifetime of the job (24h)

** Download pre-trained model and results from paper

Pre-trained TAMRFSITS model as well as all results from paper are available in the following [[https://doi.org/10.5281/zenodo.15582231][*Zenodo* repository]]:

#+begin_quote
MICHEL, J. (2025). Support data for paper "Temporal Attention Multi-Resolution Fusion of Satellite Image Time-Series, applied to Landsat-8 and Sentinel-2: all bands, any time, at best spatial resolution" (2.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.17474541
#+end_quote

** Using the test script

The [[file:bin/test.py]] script is a single entry point to perform inference and generate all the results from the paper, including the comparison with SoTA methods:

#+begin_src shell
$ ./test.py
usage: utils.py [-h] [--checkpoint CHECKPOINT] [--config CONFIG] --ts TS [TS ...] --output OUTPUT [--width WIDTH] [--seed SEED] [--device DEVICE] [--profile] [--margin MARGIN]
                [--disable_metrics] [--invert_reference_masks] [--patch_idx PATCH_IDX] [--subtile_width SUBTILE_WIDTH] [--show_subtile_progress]
                [--algorithm {TAMRFSITS,DSEN2,DEEPHARMO,STAIR,SEN2LIKE,NAIVE,DSTFN,DMS,UTILISE}]
                [--strategy {ALL,RANDOM,GAPS,NOHR,NOLR,FORECAST,BACKCAST,DEEPHARMO,ALL2CONJHR,CONJLRuHR2HR,CONJLR2HR,LRuHRNOCONJ2HR,HRNOCONJ2HR,ALLHR2ALLHR,RANDOM_ALL_DOYS,L3A,L3A_10D,CONTEXT,CUSTOM}]
                [--mask_rate_for_random_strategy MASK_RATE_FOR_RANDOM_STRATEGY] [--forecast_doy_start FORECAST_DOY_START] [--gaps_size GAPS_SIZE]
                [--context_reference_size CONTEXT_REFERENCE_SIZE] [--context_start CONTEXT_START] [--context_nb_dates CONTEXT_NB_DATES]
                [--custom_target_dates CUSTOM_TARGET_DATES [CUSTOM_TARGET_DATES ...]] [--sen2like_hpf_mtf SEN2LIKE_HPF_MTF] [--sen2like_max_masked_rate SEN2LIKE_MAX_MASKED_RATE]
                [--tamrfsits_rescomp] [--dms_tmp_dir DMS_TMP_DIR] [--dms_nb_procs DMS_NB_PROCS] [--dms_rescomp] [--write_images] [--generate_animations] [--dt_orig DT_ORIG]
utils.py: error: the following arguments are required: --ts, --output

#+end_src

A few useful options among those:

- The ~--write_images~ flag will write prediction and reference TIF files in the output folder. It is useful to visualize results using Qgis for instance, but is not recommended if more than one AOI is passed to the ~--ts~ flag (it will generate a lot of data),
- The ~--generate_animation~ flag will generate a self-contained HTML animation on top of that. Again, it is not recommended when testing multiple AOIs using the ~--ts~ flag, because of the output size and extra processing time,
- By default, the test script will use the GPU if there is one available. To force CPU execution, use ~--device cpu~.
- The ~TORCH_NUM_THREADS~ environment variable allows to limit the number of threads used by PyTorch when performing CPU inference.


Here is an example of CPU execution generating predictions every 1st and 15th of each month in 2022 (similar to the L3A example in the paper), on CPU, for AOI ~31TCJ_12~:

#+begin_src shell
TORCH_NUM_THREADS=4 PYTHONOPTIMIZE=TRUE PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True ./test.py
   --ts path_to_ls2s2/test/31TCJ_12/ # download from https://doi.org/10.5281/zenodo.15471890
   --output  path_to_output_folder
   --algorithm TAMRFSITS # Select TAMRFSITS algorithms
   --checkpoint path_to_pretrained_model/968639_s1t3.ckpt # Download from https://doi.org/10.5281/zenodo.17474541
   --config path_to_pretrained_model/hydra_config/ # Downloa from https://doi.org/10.5281/zenodo.17474541
   --strategy L3A  # Predict everty 1st and 15th of month
   --show_subtile_progress # Show intermediate progress
   --write_images # Write output images
   --disable_metrics # Disable metrics computation
   --device cpu # Perform inference on CPU
   --width 1650 # Limit the AOI to a 1650x1650 meter square
   --subtile_width 165 # Process by subtiles of 165x165 pixels
#+end_src

Please beware that CPU (and GPU) computations are memory intensive. The above example peaks at about 10 Go of RAM usage.

*** Reproducing the results from the paper

**** Gap-Filling

The following generates results for the *naive* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output gap_filling/naive --algorithm NAIVE --strategy GAPS --gaps_size 30
#+end_src

The following generates results for *TAMRF*:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output gap_filling/tamrfsits --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy GAPS --gaps_size 30
#+end_src

The following generates results for *U-TILISE*:
#+begin_src shell
export UTILISE_CONFIGURATION_FILE=/path/to/demo.yaml # Download from https://github.com/prs-eth/U-TILISE
export UTILISE_WEIGHTS_FILE=/path/to/utilise_earthnet2021.pth # Download from https://github.com/prs-eth/U-TILISE
python test.py --ts ls2s2/data/test/* --output gap_filling/utilise --algorithm UTILISE --strategy GAPS --gaps_size 30
#+end_src


**** Band-Sharpening

The following generates results for the *Dsen2* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output band_sharpening/dsen2 --algorithm DSEN2
#+end_src

The following generates results for *TAMRF*:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output gap_filling/tamrfsits --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy ALLHR2ALLHR --subtile_width 110
#+end_src

**** Spatio-Temporal Fusion

The following generates results for the *STAIR* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/stair --algorithm STAIR
#+end_src

The following generates results for the *Sen2Like* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/sen2like --algorithm SEN2LIKE --sen2like_hpf_mtf: 0.1
#+end_src

The following generates results for the *DeepHarmo* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/deepharmo --algorithm DEEPHARMO
#+end_src

The following generates results for the *DSTFN* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/dstfn --algorithm DSTFN
#+end_src

The following generates results for the *TAMRF* algorithm, with *only LR images*:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/tamrfsits_only_lr --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy CONJLR2HR
#+end_src

The following generates results for the *TAMRF* algorithm, with *only HR images*:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/tamrfsits_only_hr --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy HRNOCONJ2HR
#+end_src

The following generates results for the *TAMRF* algorithm, with *all images*:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output stf/tamrfsits_full --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy LRuHRNOCONJ2HR
#+end_src

**** Thermal Sharpening

The following generates results for the *DMS* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output thermal_sharpening/dsen2 --algorithm DMS --dms_nb_procs 16 --dms_tmp_dir /tmp/ --dms_rescomp
#+end_src

Use the  ~--dms_rescomp~ flag to activate residuals compensation.

The following generates results for the *TAMRF* algorithm:
#+begin_src shell
python test.py --ts ls2s2/data/test/* --output thermal_sharpening/tamrfsits --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy ALL
#+end_src

Use the ~--tamrfsits_rescomp~ flag to activate residuals compensation.

**** Ablation studies

The following commands allow to train the model for the ablation studies:

#+begin_src shell
$ train.py experiments=train_ls2s2_all hr_encoder.model.model.num_basic_blocks=2 lr_encoder.model.model.num_basic_blocks=2 label=baseline_s2t3

$ train.py experiments=train_ls2s2_all hr_encoder.model.model.num_basic_blocks=3 lr_encoder.model.model.num_basic_blocks=3 label=baseline_s3t3
Submitted batch job 991224

$ train.py experiments=train_ls2s2_all training_module.training_module.encoder.nb_layers=2 label=baseline_s1t2

$ train.py experiments=train_ls2s2_all training_module.training_module.encoder.nb_layers=1 label=baseline_s1t1

$ train.py experiments=ablation_lpips

$ train.py experiments=ablation_contrastive

$ train.py experiments=ablation_mae

$ train.py experiments=ablation_sensor_tokens
#+end_src


**** Level 3A synthesis

The following allows to generate the L3A example in the Perspectives of the paper:
#+begin_src shell
python test.py --ts ls2s2/data/test/test/31TCJ_12/ --output l3a/ --algorithm TAMRFSITS --checkpoint 968639_s1t3.ckpt --config hydra_config/ --strategy L3A --write_images
#+end_src



* Credits

- This work was partly performed using HPC resources from GENCI-IDRIS
  (Grant 2023-AD010114835)
- This work was partly performed using HPC resources from CNES.
- The authors acknowledge funding from the EvoLand project (Evolution
  of the Copernicus Land Service portfolio, grant agreement
  No 101082130) funded from the European Union's Horizon Europe
  research and innovation programme.
